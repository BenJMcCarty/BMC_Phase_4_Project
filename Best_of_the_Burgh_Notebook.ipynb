{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best of the 'Burgh: Forecasting Short-Term Home Sale Prices in Pittsburgh, PA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **Phase 4 Project:** Time Series Forecasting\n",
    ">\n",
    ">\n",
    "> **Author:** Benjamin McCarty\n",
    ">\n",
    ">\n",
    "> **Repository:** https://github.com/BenJMcCarty/BMC_Phase_4_Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Best-of-the-'Burgh:-Forecasting-Short-Term-Home-Sale-Prices-in-Pittsburgh,-PA\" data-toc-modified-id=\"Best-of-the-'Burgh:-Forecasting-Short-Term-Home-Sale-Prices-in-Pittsburgh,-PA-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Best of the 'Burgh: Forecasting Short-Term Home Sale Prices in Pittsburgh, PA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Business-Question\" data-toc-modified-id=\"Business-Question-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Business Question</a></span></li></ul></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Imports</a></span><ul class=\"toc-item\"><li><span><a href=\"#Reading-Data\" data-toc-modified-id=\"Reading-Data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Reading Data</a></span></li><li><span><a href=\"#Creating-Subset-of-Zipcodes\" data-toc-modified-id=\"Creating-Subset-of-Zipcodes-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Creating Subset of Zipcodes</a></span></li></ul></li><li><span><a href=\"#Data-Cleaning-and-Prep\" data-toc-modified-id=\"Data-Cleaning-and-Prep-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Cleaning and Prep</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Visualizations\" data-toc-modified-id=\"Data-Visualizations-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Data Visualizations</a></span></li></ul></li><li><span><a href=\"#Train/Test-Split\" data-toc-modified-id=\"Train/Test-Split-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Train/Test Split</a></span></li><li><span><a href=\"#Stationarity-Check\" data-toc-modified-id=\"Stationarity-Check-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Stationarity Check</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dickey-Fuller-Test\" data-toc-modified-id=\"Dickey-Fuller-Test-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Dickey Fuller Test</a></span></li><li><span><a href=\"#Removing-Trends,-Seasonality\" data-toc-modified-id=\"Removing-Trends,-Seasonality-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Removing Trends, Seasonality</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Results</a></span></li></ul></li><li><span><a href=\"#ACF/PACF-Check\" data-toc-modified-id=\"ACF/PACF-Check-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>ACF/PACF Check</a></span></li><li><span><a href=\"#SARIMA-Modeling-and-Forecasting\" data-toc-modified-id=\"SARIMA-Modeling-and-Forecasting-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>SARIMA Modeling and Forecasting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Auto-ARIMA\" data-toc-modified-id=\"Auto-ARIMA-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Auto-ARIMA</a></span></li><li><span><a href=\"#Fit-Best-Model\" data-toc-modified-id=\"Fit-Best-Model-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Fit Best Model</a></span></li><li><span><a href=\"#Forecasting\" data-toc-modified-id=\"Forecasting-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Forecasting</a></span></li></ul></li><li><span><a href=\"#Interpreting-Results\" data-toc-modified-id=\"Interpreting-Results-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Interpreting Results</a></span></li><li><span><a href=\"#Functionalizing-Workflow\" data-toc-modified-id=\"Functionalizing-Workflow-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Functionalizing Workflow</a></span></li><li><span><a href=\"#Processing-Remaining-Zip-Codes\" data-toc-modified-id=\"Processing-Remaining-Zip-Codes-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Processing Remaining Zip Codes</a></span></li><li><span><a href=\"#Inspecting-Dictionary-Results\" data-toc-modified-id=\"Inspecting-Dictionary-Results-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Inspecting Dictionary Results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Diagnosing-Zip-Code-Forecasts\" data-toc-modified-id=\"Diagnosing-Zip-Code-Forecasts-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Diagnosing Zip Code Forecasts</a></span></li><li><span><a href=\"#Creating-Groups-for-Adjustments\" data-toc-modified-id=\"Creating-Groups-for-Adjustments-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>Creating Groups for Adjustments</a></span></li><li><span><a href=\"#Review---'thresh_a025'\" data-toc-modified-id=\"Review---'thresh_a025'-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;</span>Review - <code>'thresh_a025'</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Interpretation---'thresh_a025'\" data-toc-modified-id=\"Interpretation---'thresh_a025'-11.3.1\"><span class=\"toc-item-num\">11.3.1&nbsp;&nbsp;</span>Interpretation - <code>'thresh_a025'</code></a></span></li></ul></li><li><span><a href=\"#Review---'thresh_s05'\" data-toc-modified-id=\"Review---'thresh_s05'-11.4\"><span class=\"toc-item-num\">11.4&nbsp;&nbsp;</span>Review - <code>'thresh_s05'</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Interpretation---'thresh_s05'\" data-toc-modified-id=\"Interpretation---'thresh_s05'-11.4.1\"><span class=\"toc-item-num\">11.4.1&nbsp;&nbsp;</span>Interpretation - <code>'thresh_s05'</code></a></span></li></ul></li><li><span><a href=\"#Review---'thresh_s075'\" data-toc-modified-id=\"Review---'thresh_s075'-11.5\"><span class=\"toc-item-num\">11.5&nbsp;&nbsp;</span>Review - <code>'thresh_s075'</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Interpretation---'thresh_s075'\" data-toc-modified-id=\"Interpretation---'thresh_s075'-11.5.1\"><span class=\"toc-item-num\">11.5.1&nbsp;&nbsp;</span>Interpretation - <code>'thresh_s075'</code></a></span></li></ul></li><li><span><a href=\"#Review---Zipcode-15226\" data-toc-modified-id=\"Review---Zipcode-15226-11.6\"><span class=\"toc-item-num\">11.6&nbsp;&nbsp;</span>Review - Zipcode <code>15226</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Interpretation----Zip-Code-15226\" data-toc-modified-id=\"Interpretation----Zip-Code-15226-11.6.1\"><span class=\"toc-item-num\">11.6.1&nbsp;&nbsp;</span>Interpretation  - Zip Code <code>15226</code></a></span></li></ul></li></ul></li><li><span><a href=\"#Updating-Thresholds\" data-toc-modified-id=\"Updating-Thresholds-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Updating Thresholds</a></span><ul class=\"toc-item\"><li><span><a href=\"#Updating---'thresh_a025'\" data-toc-modified-id=\"Updating---'thresh_a025'-12.1\"><span class=\"toc-item-num\">12.1&nbsp;&nbsp;</span>Updating - <code>'thresh_a025'</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Reviewing-Changes----thresh_a025\" data-toc-modified-id=\"Reviewing-Changes----thresh_a025-12.1.1\"><span class=\"toc-item-num\">12.1.1&nbsp;&nbsp;</span>Reviewing Changes  - <code>thresh_a025</code></a></span></li></ul></li><li><span><a href=\"#Updating-'thresh_s05'\" data-toc-modified-id=\"Updating-'thresh_s05'-12.2\"><span class=\"toc-item-num\">12.2&nbsp;&nbsp;</span>Updating <code>'thresh_s05'</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Reviewing-Changes----thresh_s05\" data-toc-modified-id=\"Reviewing-Changes----thresh_s05-12.2.1\"><span class=\"toc-item-num\">12.2.1&nbsp;&nbsp;</span>Reviewing Changes  - <code>thresh_s05</code></a></span></li></ul></li><li><span><a href=\"#Updating-'thresh_s075'\" data-toc-modified-id=\"Updating-'thresh_s075'-12.3\"><span class=\"toc-item-num\">12.3&nbsp;&nbsp;</span>Updating <code>'thresh_s075'</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Reviewing-Changes----thresh_s075\" data-toc-modified-id=\"Reviewing-Changes----thresh_s075-12.3.1\"><span class=\"toc-item-num\">12.3.1&nbsp;&nbsp;</span>Reviewing Changes  - <code>thresh_s075</code></a></span></li><li><span><a href=\"#Updating-thresh_s075---.725\" data-toc-modified-id=\"Updating-thresh_s075---.725-12.3.2\"><span class=\"toc-item-num\">12.3.2&nbsp;&nbsp;</span>Updating <code>thresh_s075</code> - .725</a></span></li><li><span><a href=\"#Reviewing-Changes-v2----thresh_s075\" data-toc-modified-id=\"Reviewing-Changes-v2----thresh_s075-12.3.3\"><span class=\"toc-item-num\">12.3.3&nbsp;&nbsp;</span>Reviewing Changes v2  - <code>thresh_s075</code></a></span></li></ul></li></ul></li><li><span><a href=\"#Forecast-Results\" data-toc-modified-id=\"Forecast-Results-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Forecast Results</a></span></li><li><span><a href=\"#Best-Zip-Codes\" data-toc-modified-id=\"Best-Zip-Codes-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Best Zip Codes</a></span></li><li><span><a href=\"#Final-Recommendations\" data-toc-modified-id=\"Final-Recommendations-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Final Recommendations</a></span></li><li><span><a href=\"#Future-Work\" data-toc-modified-id=\"Future-Work-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span>Future Work</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    ">If a home buyer is interested in buying a house as a short-term investment (selling within 1.5-2 years) they want to maximize their return on investment while minimizing the risk of losing money. Given a prospective buyer interested in the Pittsburgh area, they would need to know what areas are showing the strongest growth in terms of sale prices to make their decision.\n",
    ">\n",
    ">\n",
    "> Using Zillow data from 2008-2018, I forecasted sale prices and calculated the return on investment (ROI) for 19 zip codes in the Pittsburgh area. I utilized time series modeling techniques to analyze price trends over the ten-year period, then forecasted prices for the next 16 months.\n",
    ">\n",
    ">\n",
    "> The results showed three neighborhoods with both the top ROI and lowest risk of losing money. Furthermore, I identified two additional recommendations with high ROIs and two with lower risks. I recommend focusing on the top three zipcodes for best results; the additional recommendations either include a high risk of losing money or poor expected ROI.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:32.190622Z",
     "start_time": "2021-10-13T21:51:29.233506Z"
    }
   },
   "outputs": [],
   "source": [
    "## Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Visualizations\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Time Series Modeling\n",
    "import statsmodels\n",
    "import statsmodels.tsa.api as tsa\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "import pmdarima as pmd\n",
    "from pmdarima.arima import ndiffs\n",
    "from pmdarima.arima import nsdiffs\n",
    "\n",
    "## Custom-made Functions\n",
    "from bmc_functions import eda\n",
    "from bmc_functions import time_series_modeling as tsm\n",
    "\n",
    "## Settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.2f}')\n",
    "pd.set_option('max_rows', 100)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:32.332623Z",
     "start_time": "2021-10-13T21:51:32.193635Z"
    }
   },
   "outputs": [],
   "source": [
    "# mpl.rcParams[\"figure.figsize\"] = (8,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:33.263621Z",
     "start_time": "2021-10-13T21:51:32.335633Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Reading data\n",
    "source = './data/zillow_data.csv'\n",
    "data = pd.read_csv(source)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:33.403622Z",
     "start_time": "2021-10-13T21:51:33.265627Z"
    }
   },
   "outputs": [],
   "source": [
    "## Initial inspection\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Subset of Zipcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The dataset is much larger than I need for my purposes, so I will select only the zip codes for the Pittsburgh Metro area.\n",
    ">\n",
    ">\n",
    "> To select this data, I will filter the initial dataframe by selecting \"Pittsburgh\" from the \"city\" column.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:33.764623Z",
     "start_time": "2021-10-13T21:51:33.405626Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Selecting the city of Pittsburgh \n",
    "pitt_df = data[data['City'] == 'Pittsburgh']\n",
    "pitt_df = pitt_df.reset_index(drop=True)\n",
    "pitt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:33.891624Z",
     "start_time": "2021-10-13T21:51:33.767633Z"
    }
   },
   "outputs": [],
   "source": [
    "pitt_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:34.830624Z",
     "start_time": "2021-10-13T21:51:33.894623Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Examining statistics for the new dataframe\n",
    "eda.report_df(pitt_df).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The dataset currently contains monthly sale price data as columns for each zip code. In order to be able to use the sale pricing, I will use a custom function provided as part of this project to convert the year/month column label into a new single column.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:34.958622Z",
     "start_time": "2021-10-13T21:51:34.833624Z"
    }
   },
   "outputs": [],
   "source": [
    "def melt_data(df):\n",
    "    \"\"\"\n",
    "    Takes the zillow_data dataset in wide form or a subset of the zillow_dataset.  \n",
    "    Returns a long-form datetime dataframe with the datetime column names\n",
    "    as the index and the values as the 'values' column.\n",
    "    \n",
    "    If more than one row is passes in the wide-form dataset, the values column\n",
    "    will be the mean of the values from the datetime columns in all of the rows.\n",
    "    \n",
    "    Source: https://github.com/learn-co-curriculum/dsc-phase-4-project/blob/\n",
    "    main/time-series/starter_notebook.ipynb\n",
    "    \"\"\"\n",
    "    \n",
    "    melted = pd.melt(df, id_vars=['RegionName', 'RegionID', 'SizeRank','City',\n",
    "                                  'State', 'Metro', 'CountyName'],\n",
    "                     var_name='time')\n",
    "    melted['time'] = pd.to_datetime(melted['time'], infer_datetime_format=True)\n",
    "    melted = melted.dropna(subset=['value'])\n",
    "    \n",
    "    return melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:35.118621Z",
     "start_time": "2021-10-13T21:51:34.961624Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Melting the dataframe to move the dates from columns to new rows per zipcode\n",
    "pitt_melted = melt_data(pitt_df)\n",
    "pitt_melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:35.226623Z",
     "start_time": "2021-10-13T21:51:35.121625Z"
    }
   },
   "outputs": [],
   "source": [
    "## Confirming conversion to \"datetime\" datatype\n",
    "pitt_melted['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:35.338624Z",
     "start_time": "2021-10-13T21:51:35.228623Z"
    }
   },
   "outputs": [],
   "source": [
    "## Selecting columns to keep for modeling\n",
    "keep = ['RegionName', 'time', 'value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:35.450623Z",
     "start_time": "2021-10-13T21:51:35.340623Z"
    }
   },
   "outputs": [],
   "source": [
    "## Keeping only modeling-relevant data\n",
    "pitt_data = pitt_melted[keep]\n",
    "pitt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:35.594624Z",
     "start_time": "2021-10-13T21:51:35.453624Z"
    }
   },
   "outputs": [],
   "source": [
    "## Setting datetime index (required for modeling)\n",
    "pitt_data.set_index('time', inplace=True)\n",
    "pitt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> *The following code to create the final dataframe is adapted from code within [this notebook](https://github.com/flatiron-school/Online-DS-FT-022221-Cohort-Notes/blob/master/Phase_4/topic_37_intro_to_time_series/topic_37_intro_to_time_series_crime_v3-SG.ipynb) by James Irving, Ph.D.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:35.704624Z",
     "start_time": "2021-10-13T21:51:35.596625Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating list of unique zipcodes from the dataframe\n",
    "zipcodes = list(pitt_data['RegionName'].unique())\n",
    "zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:35.816623Z",
     "start_time": "2021-10-13T21:51:35.706625Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting first zipcode in list - datetime index and associated sell value\n",
    "test_code = zipcodes[0]\n",
    "test_zipcode_series = pitt_data.groupby('RegionName')\\\n",
    "                                .get_group(test_code)['value']\\\n",
    "                                                            .rename(test_code)\n",
    "test_zipcode_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:36.040655Z",
     "start_time": "2021-10-13T21:51:35.818623Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating a dictionary to store each zipcode and its timeseries data\n",
    "\n",
    "zipcodes_dict = {}\n",
    "\n",
    "for zipcode in zipcodes:\n",
    "    \n",
    "    ## Create the series for each zipcode\n",
    "    zipcode_series = pitt_data.groupby('RegionName')\\\n",
    "                                                .get_group(zipcode)['value']\\\n",
    "                                                            .rename(zipcode)\n",
    "    \n",
    "    ## Save in zipcode dictionary\n",
    "    zipcodes_dict[zipcode] = zipcode_series.resample('MS').asfreq()\n",
    "    \n",
    "## Display the keys\n",
    "zipcodes_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:36.168628Z",
     "start_time": "2021-10-13T21:51:36.042622Z"
    }
   },
   "outputs": [],
   "source": [
    "## Confirming all zip codes are present in dictionary\n",
    "list(zipcodes_dict.keys()) == zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:36.280623Z",
     "start_time": "2021-10-13T21:51:36.170629Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting values for one key:value pair\n",
    "zipcodes_dict[15206]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:36.406666Z",
     "start_time": "2021-10-13T21:51:36.282623Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## reviewing full dataset for Pittsburgh\n",
    "zipcodes_df_full = pd.DataFrame(zipcodes_dict)\n",
    "zipcodes_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:36.534632Z",
     "start_time": "2021-10-13T21:51:36.408638Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Selecting data starting from 2008 onwards\n",
    "zipcodes_df = zipcodes_df_full.loc['2008':]\n",
    "zipcodes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Now that I created the subset of data, I will visualize the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:37.281639Z",
     "start_time": "2021-10-13T21:51:36.537624Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Visualizing data via box plots\n",
    "eda.plot_boxes(zipcodes_df, x_label='Zipcode', y_label='Sale Price ($)',\n",
    "               suptitle='Sale Prices ($) for Pittsburgh, PA');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The box plots show that many zip codes seem to fall within the range of about 50k to 200k, with a few zip codes exceeding the range (particularly 15232 - Shadyside, which I know is an upper-income, popular neighborhood).\n",
    ">\n",
    ">\n",
    "> Next, I will generate a ranking of the zip codes by their average values to visualize their pricing against each other.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:37.406629Z",
     "start_time": "2021-10-13T21:51:37.283624Z"
    }
   },
   "outputs": [],
   "source": [
    "## Determining rankings of each zip code based on average sale price\n",
    "zip_rankings = list(zipcodes_df.mean().sort_values().index)\n",
    "zip_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:38.452623Z",
     "start_time": "2021-10-13T21:51:37.411637Z"
    }
   },
   "outputs": [],
   "source": [
    "## Visualizing the average prices for the top three and lowest zip codes\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax = sns.barplot(data = zipcodes_df, order = zip_rankings, palette='colorblind')\n",
    "ax.set_xlabel('Zip Code')\n",
    "ax.set_ylabel('Price ($)')\n",
    "ax.tick_params(axis='x',labelrotation=90)\n",
    "plt.suptitle('Average Sale Prices per Zip Code', size = 18)\n",
    "plt.tight_layout();\n",
    "# plt.savefig('img/Avg_Sale_Prices.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The bar graph shows that most of the zip codes have averages close to each other, with some outliers on the higher and lower ends. This matches the observations from my box plot above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:38.594623Z",
     "start_time": "2021-10-13T21:51:38.456629Z"
    }
   },
   "outputs": [],
   "source": [
    "top_bottom_zips = zipcodes_df.loc[:, [15210,15204,15212,15228,15217,15232]]\n",
    "top_bottom_zips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> To begin the modeling process, I will split my data into a training and testing pair. This split allows me to validate my forecast data after I create my models.\n",
    ">\n",
    ">\n",
    "> As part of my EDA process, I will examine the stationarity of the training set to identify what methods would be most effective to establish stationarity during the modeling process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:38.736622Z",
     "start_time": "2021-10-13T21:51:38.596626Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Testing first zipcode from dictionary\n",
    "zipcode_val = zipcodes_df[15206].copy()\n",
    "zipcode_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:39.232653Z",
     "start_time": "2021-10-13T21:51:38.739624Z"
    }
   },
   "outputs": [],
   "source": [
    "## Visualizing first zipcode priot to split\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,4))\n",
    "ax = zipcode_val.plot()\n",
    "ax.legend()\n",
    "ax.set_xlabel('Years')\n",
    "ax.set_ylabel('Sale Price ($)')\n",
    "ax.set_title(f'Train/Test Split for Zipcode {zipcode_val.name}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:39.710625Z",
     "start_time": "2021-10-13T21:51:39.234630Z"
    }
   },
   "outputs": [],
   "source": [
    "## Splitting Data\n",
    "\n",
    "tts_cutoff = round(zipcode_val.shape[0]*.85)\n",
    "train = zipcode_val.iloc[:tts_cutoff]\n",
    "test = zipcode_val.iloc[tts_cutoff:]\n",
    "\n",
    "## Plot\n",
    "fig, ax = plt.subplots(figsize = (12,4))\n",
    "ax = train.plot(label='Train')\n",
    "ax = test.plot(label='Test')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Years')\n",
    "ax.set_ylabel('Price ($)')\n",
    "ax.set_title(f'Train/Test Split for Zipcode {zipcode_val.name}')\n",
    "ax.axvline(train.index[-1], linestyle=\":\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:40.236623Z",
     "start_time": "2021-10-13T21:51:39.713625Z"
    }
   },
   "outputs": [],
   "source": [
    "## Testing functionalized train/test split for reuse on other zipcodes`\n",
    "train, test, fig = tsm.ts_split(zipcode_val, show_vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:40.364623Z",
     "start_time": "2021-10-13T21:51:40.238623Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting training set\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:40.476653Z",
     "start_time": "2021-10-13T21:51:40.366634Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Inspecting testing set\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationarity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The following functions are adapted from [this notebook](https://github.com/flatiron-school/Online-DS-FT-022221-Cohort-Notes/blob/master/Phase_4/topic_37_intro_to_time_series/topic_37_intro_to_time_series_crime_v3-SG.ipynb) by James Irving, Ph.D.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dickey Fuller Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:40.604622Z",
     "start_time": "2021-10-13T21:51:40.479623Z"
    }
   },
   "outputs": [],
   "source": [
    "## Performing Dickey-Fuller Test\n",
    "zipdf_results = tsa.stattools.adfuller(train)\n",
    "zipdf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:40.825626Z",
     "start_time": "2021-10-13T21:51:40.606624Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating a dictionary to store initial results\n",
    "index_label =[f'{train.name}']\n",
    "labels = ['Test Stat','P-Value','Number of Lags Used','Number of Obs. Used',\n",
    "        'Critical Thresholds', 'AIC Value']\n",
    "results_dict  = dict(zip(labels,zipdf_results))\n",
    "\n",
    "## Saving results to a dictionary and adding T/F for whether exceeds standard\n",
    "## p-value of .05 and if we fail to reject the null hypothesis or not.\n",
    "results_dict['p < .05'] = results_dict['P-Value']<.05\n",
    "results_dict['Stationary'] = results_dict['p < .05']\n",
    "\n",
    "## Creating DataFrame from dictionary\n",
    "if isinstance(index_label,str):\n",
    "    index_label = [index_label]\n",
    "results_dict = pd.DataFrame(results_dict,index=index_label)\n",
    "results_dict = results_dict[['Test Stat','P-Value','Number of Lags Used',\n",
    "                             'Number of Obs. Used','P-Value','p < .05',\n",
    "                             'Stationary']]\n",
    "\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:40.967623Z",
     "start_time": "2021-10-13T21:51:40.828626Z"
    }
   },
   "outputs": [],
   "source": [
    "## Testing functionality\n",
    "tsm.adf_test(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The initial Dickey-Fuller test shows that the initial training set is not stationary. I will review a few different methods to establish stationarity below to determine the most effective way to establish stationarity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Trends, Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:41.490624Z",
     "start_time": "2021-10-13T21:51:40.970624Z"
    }
   },
   "outputs": [],
   "source": [
    "## Testing differenced data\n",
    "tz_diff = train.diff().dropna()\n",
    "print(\"|\",\"---\",f\"Zipcode {train.name}\",\"---\",\"|\",\"\\n\")\n",
    "print(tz_diff)\n",
    "print('\\n\\n',\"|\",\"----\"*5,f\"ADF Results for Zipcode {train.name}\",\"-----\"*6,\"|\")\n",
    "display(tsm.adf_test(tz_diff))\n",
    "\n",
    "print('\\n\\n','|',\"----\"*7,f\"Visualizing Difference Shift\",\"---\"*7,\"|\")\n",
    "fig, ax = plt.subplots()\n",
    "ax = tz_diff.plot(label='Post-Differencing')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Years')\n",
    "ax.set_ylabel('Price ($)')\n",
    "ax.set_title(f'Difference Shift for Zipcode {train.name}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:41.958625Z",
     "start_time": "2021-10-13T21:51:41.493626Z"
    }
   },
   "outputs": [],
   "source": [
    "## Testing functionalized process from above\n",
    "diff_results = tsm.remove_trends(train, \"diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:42.452623Z",
     "start_time": "2021-10-13T21:51:41.960624Z"
    }
   },
   "outputs": [],
   "source": [
    "## Testing a log transformation\n",
    "log_results = tsm.remove_trends(train, \"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:42.966624Z",
     "start_time": "2021-10-13T21:51:42.454622Z"
    }
   },
   "outputs": [],
   "source": [
    "## Removing the rolliing mean from the data\n",
    "rolling_results = tsm.remove_trends(train, \"rolling mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:43.457624Z",
     "start_time": "2021-10-13T21:51:42.968624Z"
    }
   },
   "outputs": [],
   "source": [
    "## Removing the exponentially-weighted mean from the data\n",
    "ewm_results = tsm.remove_trends(train, \"EWM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:44.126632Z",
     "start_time": "2021-10-13T21:51:43.459638Z"
    }
   },
   "outputs": [],
   "source": [
    "## Performing seasonal decomposition\n",
    "decomp = seasonal_decompose(train)\n",
    "decomp.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:44.302624Z",
     "start_time": "2021-10-13T21:51:44.128625Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating Dataframe with seasonality test results\n",
    "\n",
    "test_results = []\n",
    "test_results.append(tsm.adf_test(train))\n",
    "\n",
    "decomp_dict = {\"trend\": decomp.trend,'seasonal': decomp.seasonal,\n",
    "               'residuals': decomp.resid}\n",
    " \n",
    "for trend, results in decomp_dict.items():\n",
    "\n",
    "    results = results.fillna(0)\n",
    "    res = tsm.adf_test(results)\n",
    "    test_results.append(res)\n",
    "\n",
    "## make into a df\n",
    "seasonality_df = pd.concat(test_results)\n",
    "seasonality_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> After testing several methods, the only effective method to establish stationarity is to use seasonal decomposition. I will use this information for my modeling process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACF/PACF Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> For the final visualization, I will check the auto-correlation for the dataset. Ideally, the data will not show any auto-correlation past the first point.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:44.808621Z",
     "start_time": "2021-10-13T21:51:44.305625Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tsm.plot_acf_pacf(train, suptitle='ACF/PACF for Training Data', lags = 51);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMA Modeling and Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> After reviewing my data, I am ready to begin the modeling process. I will use the auto-arima function from pmdarima to determine the best parameters for my model, then use those parameters in a SARIMA model for my forecasts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:44.952624Z",
     "start_time": "2021-10-13T21:51:44.810622Z"
    }
   },
   "outputs": [],
   "source": [
    "## Using pmdarima's functions to pre-determine the best values for \n",
    "## differencing prior to running auto_arima\n",
    "\n",
    "n_d = ndiffs(train)\n",
    "\n",
    "n_D = nsdiffs(train, m=12)\n",
    "\n",
    "n_d, n_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:46.442623Z",
     "start_time": "2021-10-13T21:51:44.955624Z"
    }
   },
   "outputs": [],
   "source": [
    "## Using auto_arima to determine best parameters for modeling\n",
    "auto_model = pmd.auto_arima(train,start_p=0,start_q=0,d=n_d,\n",
    "                            max_p=3,max_q=3,\n",
    "                            max_P=3,max_Q=3, D=n_D,\n",
    "                            start_P=0,start_Q=0,\n",
    "                            m=12,\n",
    "                            verbose=2)\n",
    "\n",
    "display(auto_model.summary())\n",
    "auto_model.plot_diagnostics(figsize= (12,9));\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:46.597623Z",
     "start_time": "2021-10-13T21:51:46.444624Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model = tsa.SARIMAX(train,order=auto_model.order,\n",
    "                         seasonal_order = auto_model.seasonal_order,\n",
    "                         enforce_invertibility=False).fit()\n",
    "\n",
    "## Display Summary + Diagnostics\n",
    "display(best_model.summary())\n",
    "# best_model.plot_diagnostics()\n",
    "# plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:46.756623Z",
     "start_time": "2021-10-13T21:51:46.600628Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Using get_forecast to generate forecasted data\n",
    "forecast = best_model.get_forecast(steps=len(test))\n",
    "\n",
    "## Saving confidence intervals and predicted mean for future\n",
    "forecast_df = forecast.conf_int()\n",
    "forecast_df.columns = ['Lower CI','Upper CI']\n",
    "forecast_df['Forecast'] = forecast.predicted_mean\n",
    "forecast_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:47.216624Z",
     "start_time": "2021-10-13T21:51:46.759626Z"
    }
   },
   "outputs": [],
   "source": [
    "## Plotting training, test data and forecasted results\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "last_n_lags=12*5         \n",
    "\n",
    "train.iloc[-last_n_lags:].plot(label='Training Data')\n",
    "test.plot(label='Test Data')\n",
    "\n",
    "## Plotting forecasted data and confidence intervals\n",
    "forecast_df['Forecast'].plot(ax=ax,label='Forecast')\n",
    "ax.fill_between(forecast_df.index,forecast_df['Lower CI'],\n",
    "                forecast_df['Upper CI'],color='y',alpha=0.4)\n",
    "\n",
    "ax.set(xlabel='Time')\n",
    "ax.set(ylabel='Sell Price ($)')\n",
    "ax.set_title('Data and Forecasted Data')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:47.744635Z",
     "start_time": "2021-10-13T21:51:47.219634Z"
    }
   },
   "outputs": [],
   "source": [
    "## Testing functionalization of forecasting process\n",
    "fig = tsm.plot_forecast_ttf(train, test,forecast_df = forecast_df, figsize = (12,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:48.192624Z",
     "start_time": "2021-10-13T21:51:47.746623Z"
    }
   },
   "outputs": [],
   "source": [
    "fig['figure']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Now that I have a baseline forecast for my test zip code, I will create predictions on the full data for that zip code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.206653Z",
     "start_time": "2021-10-13T21:51:48.194627Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model = tsa.SARIMAX(zipcode_val,order=auto_model.order,\n",
    "                         seasonal_order = auto_model.seasonal_order,\n",
    "                         enforce_invertibility=False).fit()\n",
    "\n",
    "display(best_model.summary())\n",
    "best_model.plot_diagnostics(figsize=(12,6));\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.211626Z",
     "start_time": "2021-10-13T21:51:29.456Z"
    }
   },
   "outputs": [],
   "source": [
    "auto_model_best, best_model_overall = tsm.create_best_model(zipcode_val, m=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.212623Z",
     "start_time": "2021-10-13T21:51:29.461Z"
    }
   },
   "outputs": [],
   "source": [
    "## Using get_forecast to generate forecasted data\n",
    "forecast = best_model.get_forecast(steps=24)\n",
    "\n",
    "## Saving confidence intervals and predicted mean for future\n",
    "forecast_df = forecast.conf_int()\n",
    "forecast_df.columns = ['Lower CI','Upper CI']\n",
    "forecast_df['Forecast'] = forecast.predicted_mean\n",
    "forecast_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.213623Z",
     "start_time": "2021-10-13T21:51:29.465Z"
    }
   },
   "outputs": [],
   "source": [
    "## Plotting training, test data and forecasted results\n",
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "zipcode_val.plot(label='Training Data',figsize=(10,5))\n",
    "\n",
    "## Plotting forecasted data and confidence intervals\n",
    "forecast_df['Forecast'].plot(ax=ax,label='Forecast')\n",
    "ax.fill_between(forecast_df.index,forecast_df['Lower CI'],\n",
    "                forecast_df['Upper CI'],color='b',alpha=0.4)\n",
    "\n",
    "ax.set(xlabel='Time')\n",
    "ax.set(ylabel='Sale Price ($)')\n",
    "ax.set_title('Data and Forecasted Data')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.215623Z",
     "start_time": "2021-10-13T21:51:29.469Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forecast_overall = tsm.forecast_and_ci(best_model_overall, test)\n",
    "forecast_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.216624Z",
     "start_time": "2021-10-13T21:51:29.473Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test= tsm.plot_forecast_final(zipcode_val, forecast_overall, figsize=(10,5))\n",
    "test['figure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.218624Z",
     "start_time": "2021-10-13T21:51:29.477Z"
    }
   },
   "outputs": [],
   "source": [
    "investment_cost = forecast_df.iloc[0,2]\n",
    "investment_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.219638Z",
     "start_time": "2021-10-13T21:51:29.482Z"
    }
   },
   "outputs": [],
   "source": [
    "roi_df = (forecast_df - investment_cost)/investment_cost*100\n",
    "roi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.220625Z",
     "start_time": "2021-10-13T21:51:29.486Z"
    }
   },
   "outputs": [],
   "source": [
    "roi_final = roi_df.iloc[-1]\n",
    "roi_final.name = zipcode_val.name.astype('str')\n",
    "roi_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.222624Z",
     "start_time": "2021-10-13T21:51:29.491Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(roi_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Based on my model, the ROI for the zipcode 15206 would be an average of 65.48%. However, the results may fall anywhere between 19.05% - 111.91%.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functionalizing Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.224624Z",
     "start_time": "2021-10-13T21:51:29.498Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test, split_fig = tsm.ts_split(zipcodes_df[15206], .85, show_vis = True, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.226626Z",
     "start_time": "2021-10-13T21:51:29.502Z"
    }
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.227624Z",
     "start_time": "2021-10-13T21:51:29.506Z"
    }
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.228624Z",
     "start_time": "2021-10-13T21:51:29.510Z"
    }
   },
   "outputs": [],
   "source": [
    "split_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.230623Z",
     "start_time": "2021-10-13T21:51:29.513Z"
    }
   },
   "outputs": [],
   "source": [
    "auto, best = tsm.create_best_model(zipcodes_df[15206])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.231623Z",
     "start_time": "2021-10-13T21:51:29.518Z"
    }
   },
   "outputs": [],
   "source": [
    "forecast_df = tsm.forecast_and_ci(best,test)\n",
    "forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.233628Z",
     "start_time": "2021-10-13T21:51:29.521Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test, split_fig = tsm.ts_split(zipcodes_df[15206])\n",
    "figsize = (10,5)\n",
    "fig,ax=plt.subplots(figsize = figsize)\n",
    "show_vis = True\n",
    "\n",
    "# train = split_dict.get('train')\n",
    "# test = split_dict.get('test')\n",
    "\n",
    "last_n_lags=len(train)\n",
    "\n",
    "train.iloc[-last_n_lags:].plot(label='Training Data', ax=ax)\n",
    "test.plot(label='Test Data', ax=ax)\n",
    "\n",
    "## Plotting forecasted data and confidence intervals\n",
    "forecast_df['Forecast'].plot(label='Forecast', color='g', ax=ax)\n",
    "ax.fill_between(forecast_df.index,forecast_df['Lower CI'],\n",
    "                forecast_df['Upper CI'],color='y',alpha=0.275)\n",
    "ax.set(xlabel='Years')\n",
    "ax.set(ylabel='Sale Price ($)')\n",
    "ax.set_title(f'Zipcode {train.name}: Validating Forecasted Data')\n",
    "ax.axvline(test.index[0], linestyle=\":\",\n",
    "    label=f'Beginning of Forecast: {test.index[0].year} - {test.index[0].month}',\n",
    "    color='k')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "ttf_dict = {}\n",
    "ttf_dict['figure'] = fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.235625Z",
     "start_time": "2021-10-13T21:51:29.527Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Testing full workflow function\n",
    "\n",
    "tsa_results = tsm.ts_modeling_workflow(dataframe = zipcodes_df, zipcode = 15206, m=12, show_vis = False, figsize=(12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Remaining Zip Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Now I will process the remaining zip codes via a for loop to process them through the work flow. As part of the work flow, I will review each model's performance visualizations to ensure it is appropriate for forecasting.\n",
    ">\n",
    ">\n",
    ">I will save the results to the overall dictionary for my final review and interpretation.\n",
    "\n",
    "---\n",
    "\n",
    "> ***Special Note:*** Before looping through the entirety of the zip codes, I remove the zip code \"15210\" and process it separately.\n",
    ">\n",
    ">\n",
    "> This is due to significant delays in running the loop (increasing loop runtime from 1.5 minutes to upwards of 10+ minutes). The issue stems from errors during the modeling process when using the default train/test split threshold of .85.\n",
    ">\n",
    ">\n",
    "> To resolve the issue, I run the zip code through the same process as the loop, adjusting the threshold specifically for this zip code. Then, I save the results to the overall dictionary.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.236625Z",
     "start_time": "2021-10-13T21:51:29.533Z"
    }
   },
   "outputs": [],
   "source": [
    "## Separating the 15210 zipcode to prevent runtime delays\n",
    "shorter_list = list(zipcodes_df.columns)\n",
    "shorter_list.remove(15210)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.240627Z",
     "start_time": "2021-10-13T21:51:29.537Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Creating dictionary and storing all zipcodes and results\n",
    "overall_results = {}\n",
    "\n",
    "for i, zipcode in enumerate(shorter_list):\n",
    "    \n",
    "    print('|',\"---\"*10,f'Zipcode {zipcode}',\"---\"*10,'|\\n')\n",
    "    print(f'--> Zipcode {i+1} of {len(shorter_list)}')\n",
    "    \n",
    "    tsa_results = {}\n",
    "    metrics = {}\n",
    "    forecast_vis = {}\n",
    "\n",
    "    ## Select values for the selected zipcode\n",
    "    zipcode_val = zipcodes_df[zipcode].copy()\n",
    "\n",
    "    print('| --- Train/Test Split Visualization --- |')\n",
    "    ## Split dataset\n",
    "    train, test, split_fig = tsm.ts_split(zipcode_val, .85, show_vis = True)\n",
    "    plt.show(split_fig)\n",
    "    plt.close()\n",
    "    \n",
    "    ## Generating auto_arima model and SARIMAX model\n",
    "    ## (based on best parameters from auto_arima model)\n",
    "    auto_model_train, best_model_train = tsm.create_best_model(timeseries_dataset = train, m=12)\n",
    "\n",
    "    ## Saving training model results\n",
    "    metrics['train'] = tsm.model_performance(best_model_train)\n",
    "#     plt.show(metrics['train'])\n",
    "    plt.close()\n",
    "\n",
    "    ## Generating dataframe to store forecast results\n",
    "    forecast_train = tsm.forecast_and_ci(best_model_train, test)\n",
    "\n",
    "    print('| --- Validation Model Visualization --- |')\n",
    "    \n",
    "    ## Plotting forecast results against train/test split\n",
    "    forecast_vis['train'] = tsm.plot_forecast_ttf(train, test, forecast_df = forecast_train, show_vis=True)\n",
    "    plt.show(forecast_vis['train'])\n",
    "    plt.close()\n",
    "    \n",
    "    ## Fitting best model using whole dataset\n",
    "    best_model_full = tsa.SARIMAX(zipcode_val,order=auto_model_train.order,\n",
    "                            seasonal_order = auto_model_train.seasonal_order,\n",
    "                            enforce_invertibility=False).fit()\n",
    "\n",
    "    print('| --- Final Model Diagnostics --- |')\n",
    "    \n",
    "    metrics['full'] = tsm.model_performance(best_model_full)\n",
    "    plt.show(metrics['full'])\n",
    "    plt.close()\n",
    "    \n",
    "    ## Using get_forecast to generate forecasted data\n",
    "    tsa_results['forecasted_prices'] = tsm.forecast_and_ci(best_model_full, test)\n",
    "\n",
    "    print('| --- Final Forecast --- |')\n",
    "    ## Plotting original data and forecast results\n",
    "    forecast_vis['full'] = tsm.plot_forecast_final(zipcode_val, tsa_results['forecasted_prices'], show_vis = True)\n",
    "    plt.show(forecast_vis['full'])\n",
    "    plt.close()\n",
    "    \n",
    "    ## Calculating investment cost and ROI across dataframe\n",
    "    investment_cost = tsa_results['forecasted_prices'].iloc[0,2]\n",
    "    tsa_results['roi'] = (tsa_results['forecasted_prices'] - investment_cost)/investment_cost*100\n",
    "    \n",
    "    tsa_results['num_yrs_forecast'] = len(test)\n",
    "    tsa_results['model_metrics'] = metrics\n",
    "    tsa_results['model_visuals'] = forecast_vis\n",
    "    \n",
    "    ## Save final temporary dictionary to overall dictionary\n",
    "    overall_results[zipcode] = tsa_results\n",
    "    \n",
    "    print(f'--> Zipcode {i+1} of {len(shorter_list)}')\n",
    "    print('|',\"---\"*5,f'Completed: {zipcode}',\"---\"*5,'|\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.242641Z",
     "start_time": "2021-10-13T21:51:29.541Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Processing zipcode 15210 separately\n",
    "overall_results[15210] = tsm.ts_modeling_workflow(zipcodes_df, 15210, .8, show_vis = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting Dictionary Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    ">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.244623Z",
     "start_time": "2021-10-13T21:51:29.547Z"
    }
   },
   "outputs": [],
   "source": [
    "overall_results[15206].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.246625Z",
     "start_time": "2021-10-13T21:51:29.551Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting \"forecasted prices\" key\n",
    "overall_results[15206]['forecasted_prices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.247623Z",
     "start_time": "2021-10-13T21:51:29.556Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting \"roi\" key\n",
    "overall_results[15206]['roi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.249629Z",
     "start_time": "2021-10-13T21:51:29.563Z"
    }
   },
   "outputs": [],
   "source": [
    "overall_results[15206]['num_yrs_forecast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.250629Z",
     "start_time": "2021-10-13T21:51:29.568Z"
    }
   },
   "outputs": [],
   "source": [
    "overall_results[15206]['model_metrics'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.251625Z",
     "start_time": "2021-10-13T21:51:29.576Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Reviewing training model metrics\n",
    "display(overall_results[15206]['model_metrics']['train']['summary'])\n",
    "display(overall_results[15206]['model_metrics']['full']['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.253626Z",
     "start_time": "2021-10-13T21:51:29.582Z"
    }
   },
   "outputs": [],
   "source": [
    "display(overall_results[15206]['model_visuals']['train']['figure'])\n",
    "display(overall_results[15206]['model_visuals']['full']['figure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing Zip Code Forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> After generating the forecast results for all of the zip codes, I reviewed the validation results for each zip code.\n",
    ">\n",
    ">\n",
    "> Certain zip codes showed the actual sale price trend lines getting too close to the upper/lower confidence intervals. Several models missed the trends entirely, resulting in the actual data exceeding the confidence interval.\n",
    ">\n",
    ">\n",
    "> **I will readjust the train/test threshold for the selected zip codes to address these issues.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Groups for Adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.254625Z",
     "start_time": "2021-10-13T21:51:29.589Z"
    }
   },
   "outputs": [],
   "source": [
    "## Adding .025 to threshold\n",
    "thresh_a025 = [15217,15213,15216]\n",
    "\n",
    "## Subtracting .05 from threshold\n",
    "thresh_s05 = [15243]\n",
    "\n",
    "## Subtracting .075 from threshold\n",
    "thresh_s075 = [15210, 15207, 15204]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review - `'thresh_a025'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.256625Z",
     "start_time": "2021-10-13T21:51:29.595Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Inspecting split and validation visuals for missed trends\n",
    "\n",
    "for code in thresh_a025:\n",
    "    print(\"\\n|\",\"--\"*24,f\"Visualizations for {code}\",\"--\"*24,\"|\\n\")\n",
    "#     display(overall_results[code]['model_visuals']['split'])\n",
    "    display(overall_results[code]['model_visuals']['train']['figure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation - `'thresh_a025'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> For these zip codes, I see that the train/test split threshold slightly missed a trend in the data, causing the actual results to approach one of the limits of the threshold too closely.\n",
    ">\n",
    ">\n",
    "> I will test whether **increasing the threshold slightly would capture more of the trend**, bringing my forecast data closer to the test data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review - `'thresh_s05'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.257625Z",
     "start_time": "2021-10-13T21:51:29.604Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Inspecting split and validation visuals for missed trends\n",
    "\n",
    "for code in thresh_s05:\n",
    "    print(\"\\n|\",\"--\"*24,f\"Visualizations for {code}\",\"--\"*24,\"|\\n\")\n",
    "#     display(overall_results[code]['model_visuals']['split'])\n",
    "    display(overall_results[code]['model_visuals']['train']['figure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation - `'thresh_s05'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Similar to the prior zipcodes, these zipcodes missed the trend as well. However, it seems that the trend may be *behind* the threshold.\n",
    ">\n",
    ">\n",
    "> I will test whether **decreasing the threshold by .05 would capture more of the trend.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review - `'thresh_s075'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.259624Z",
     "start_time": "2021-10-13T21:51:29.612Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Inspecting split and validation visuals for missed trends\n",
    "\n",
    "for i, zipcode in enumerate(thresh_s075):\n",
    "    \n",
    "    print('|',\"---\"*10,f'Zipcode {zipcode}',\"---\"*10,'|\\n')\n",
    "    print(f'--> Zipcode {i+1} of {len(thresh_s075)}\\n')\n",
    "    \n",
    "    ## Reviewing training model metrics\n",
    "    \n",
    "    print('|',\"---\"*5,'Model Visualizations',\"---\"*5,'|\\n')\n",
    "#     display(overall_results[zipcode]['model_visuals']['split'])\n",
    "    display(overall_results[zipcode]['model_visuals']['train']['figure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation - `'thresh_s075'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **These zipcodes missed the trends significantly as well, with forecasts exceeding the confidence intervals.** The trends may be further behind the threshold, requiring more of a reduction in the threshold.\n",
    ">\n",
    ">\n",
    "> I will test whether **decreasing the threshold by .075 would capture more of the trend.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review - Zipcode `15226`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> During my review, I noticed there was a sharp increase in the trend line for the zip code 15226, causing my model to mis-forecast the sale prices.\n",
    ">\n",
    ">\n",
    "> **In order to address this error, I would need to increase my threshold an additional 5%.** This decision would be problematic, however, as it would limit the scope of this, and all other forecasts, to a one-year scope.\n",
    ">\n",
    ">\n",
    "> **Instead of limiting all of my forecasts due to this single zip code, I will leave the model results at the .85 threshold.** \n",
    ">\n",
    ">\n",
    "> For exploratory purposes, I will visualize the impact of the change to a .90 threshold. However, **these results will not be included in my final results.**\n",
    ">\n",
    ">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.260624Z",
     "start_time": "2021-10-13T21:51:29.625Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## EDA modeling of the 15226 zip code at a .9 threshold for train/test split\n",
    "\n",
    "tsm.ts_modeling_workflow(zipcodes_df, 15226,threshold = .9, show_vis = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation  - Zip Code `15226`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> As expected, increasing the threshold did increase the accuracy of the trend for the 15226 zip code. However, this change would limit the forecasts of the other zip codes by nearly 6 months. As this is only one zipcode, I will leave it's threshold at .85 to maintain the forecasts for the others.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating Thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating - `'thresh_a025'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.262625Z",
     "start_time": "2021-10-13T21:51:29.634Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, zipcode in enumerate(thresh_a025):\n",
    "    \n",
    "    print('|',\"---\"*10,f'Zipcode {zipcode}',\"---\"*10,'|\\n')\n",
    "    print(f'--> Zipcode {i+1} of {len(thresh_a025)}')\n",
    "    \n",
    "   \n",
    "    overall_results[zipcode] = tsm.ts_modeling_workflow(zipcodes_df, zipcode,\n",
    "                                             threshold = .875, show_vis = True)\n",
    "    \n",
    "    print('|',\"---\"*3,f'Completed: {zipcode}, {i+1} of {len(thresh_a025)}',\"---\"*3,'|\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing Changes  - `thresh_a025`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The slight increase to the threshold for these zipcodes brought the forecasts much closer to the test values, in most cases making them nearly the same as the test data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating `'thresh_s05'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.263625Z",
     "start_time": "2021-10-13T21:51:29.641Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, zipcode in enumerate(thresh_s05):\n",
    "    \n",
    "    print('|',\"---\"*10,f'Zipcode {zipcode}',\"---\"*10,'|\\n')\n",
    "    print(f'--> Zipcode {i+1} of {len(thresh_s05)}')\n",
    "    \n",
    "   \n",
    "    overall_results[zipcode] = tsm.ts_modeling_workflow(zipcodes_df, zipcode,\n",
    "                                             threshold = .8, show_vis = True)\n",
    "\n",
    "    print('|',\"---\"*3,f'Completed: {zipcode}, {i+1} of {len(thresh_s05)}',\n",
    "          \"---\"*3,'|\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing Changes  - `thresh_s05`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The decrease of .05 in my threshold improved my forecasts for 15243. However, it seems my forecast for 15226 could still improve.\n",
    ">\n",
    ">\n",
    "> I will need to change the threshold again for 15226 to increase the accuracy of my forecast.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating `'thresh_s075'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.264641Z",
     "start_time": "2021-10-13T21:51:29.649Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, zipcode in enumerate(thresh_s075):\n",
    "    \n",
    "    print('|',\"---\"*10,f'Zipcode {zipcode}',\"---\"*10,'|\\n')\n",
    "    print(f'--> Zipcode {i+1} of {len(thresh_s075)}')\n",
    "    \n",
    "   \n",
    "    overall_results[zipcode] = tsm.ts_modeling_workflow(zipcodes_df, zipcode,\n",
    "                                             threshold = .775, show_vis = True)\n",
    "    \n",
    "    print('|',\"---\"*3,f'Completed: {zipcode}, {i+1} of {len(thresh_s075)}',\n",
    "          \"---\"*3,'|\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing Changes  - `thresh_s075`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The decrease of .075 in my threshold improved my forecasts for 15207, but they still have room for improvement. However, this threshold is still showing poor performance for 15210 and 15204.\n",
    ">\n",
    ">\n",
    "> I will change the threshold again for these zip codes to see if a larger decrease would improve the accuracy further.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating `thresh_s075` - .725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.265625Z",
     "start_time": "2021-10-13T21:51:29.661Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Decreasing threshold to .725\n",
    "\n",
    "for i, zipcode in enumerate(thresh_s075):\n",
    "    print('|',\"---\"*10,f'Zipcode {zipcode}',\"---\"*10,'|\\n')\n",
    "    print(f'--> Zipcode {i+1} of {len(thresh_s075)}')\n",
    "    \n",
    "   \n",
    "    overall_results[zipcode] = tsm.ts_modeling_workflow(zipcodes_df, zipcode,\n",
    "                                             threshold = .725, show_vis = True)\n",
    "    \n",
    "    print('|',\"---\"*3,f'Completed: {zipcode}, {i+1} of {len(thresh_s075)}',\n",
    "          \"---\"*3,'|\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing Changes v2  - `thresh_s075`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Bringing the threshold down to .725 from .85 brought the zip code 15207 within its confidence interval. However, zip codes 15204 and 15210 both have unstable trend lines in the training and test data, making it hard for the model to predict accurate results.\n",
    ">\n",
    ">\n",
    "> I will accept these results with the understanding that the forecast for zip codes 15204 and 15210 will be inaccurate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**ROI and Risk**\n",
    "\n",
    "> Now that I collected all of the results for each zip code, I will calculate and save the return on investment (ROI) values for each zip code. **I will determine my final recommendations based on the ROI results as well as using the lower confidence interval to represent the risk of losing money for each zip code.**\n",
    "\n",
    "**Calculating Return on Investment (ROI)**\n",
    "\n",
    "> I calculate the ROI for each month's forecast per zip code by subtracting the first forecasted price from the new price, then dividing the difference by the investment cost. Finally, I multiply the quotient by 100 to turn it into a percentage.\n",
    ">\n",
    "> The equation is as follows:\n",
    ">\n",
    "> $(newprice - first)/first*100$\n",
    ">\n",
    "> The resulting percentage indicates the percentage of the initial cost of the house as either a gain (if the percentage is positive) or a loss (if negative).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.267623Z",
     "start_time": "2021-10-13T21:51:29.672Z"
    }
   },
   "outputs": [],
   "source": [
    "## Identifying keys for each zip code\n",
    "overall_results[15206].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.268630Z",
     "start_time": "2021-10-13T21:51:29.680Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting ROI dictionary\n",
    "overall_results[15206]['roi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.271625Z",
     "start_time": "2021-10-13T21:51:29.685Z"
    }
   },
   "outputs": [],
   "source": [
    "## Calculating number of months used in each forecast\n",
    "roi_len = []\n",
    "\n",
    "for zipcode, data in overall_results.items():\n",
    "    roi_len.append(len(data['roi']))\n",
    "    \n",
    "roi_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.273627Z",
     "start_time": "2021-10-13T21:51:29.689Z"
    }
   },
   "outputs": [],
   "source": [
    "## Determining minimum number of months for comparisons\n",
    "roi_idx = min(roi_len)\n",
    "roi_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.275624Z",
     "start_time": "2021-10-13T21:51:29.696Z"
    }
   },
   "outputs": [],
   "source": [
    "## Confirming indexing works as expected\n",
    "overall_results[15206]['roi'].iloc[roi_idx-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.276622Z",
     "start_time": "2021-10-13T21:51:29.702Z"
    }
   },
   "outputs": [],
   "source": [
    "## Collecting forecasted ROI and confidence intervals\n",
    "roi_list = []\n",
    "\n",
    "for zipcode, data in overall_results.items():\n",
    "    roi_list.append(data['roi'].iloc[roi_idx-1].rename(zipcode).to_frame().T)\n",
    "    \n",
    "roi_df = pd.concat(roi_list)\n",
    "\n",
    "roi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.277622Z",
     "start_time": "2021-10-13T21:51:29.706Z"
    }
   },
   "outputs": [],
   "source": [
    "overall_results[15206]['forecasted_prices'].iloc[roi_idx-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.279625Z",
     "start_time": "2021-10-13T21:51:29.711Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Collecting forecasted ROI and confidence intervals AT THE MIN. FRCST THRSHLD\n",
    "price_list= []\n",
    "\n",
    "for zipcode, data in overall_results.items():\n",
    "    price_list.append(data['forecasted_prices']\\\n",
    "                            .iloc[roi_idx-1].rename(zipcode).to_frame().T)\n",
    "    \n",
    "price_df = pd.concat(price_list)\n",
    "\n",
    "price_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.280627Z",
     "start_time": "2021-10-13T21:51:29.717Z"
    }
   },
   "outputs": [],
   "source": [
    "## Sorting for zip codes with highest ROI\n",
    "best_roi_df = roi_df.sort_values('Forecast', ascending=False)\n",
    "best_roi_df.style.background_gradient(subset=['Forecast'],\n",
    "                                  cmap='RdYlGn')\\\n",
    "                                    .set_caption('Zipcodes by Forecasted ROI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.282623Z",
     "start_time": "2021-10-13T21:51:29.721Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Sorting zipcodes by risk\n",
    "risk_df = roi_df.sort_values('Lower CI', ascending=False)\n",
    "risk_df.style.background_gradient(subset=['Lower CI'],\n",
    "                                  cmap='RdYlGn').set_caption('Zipcodes by Risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Zip Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.283635Z",
     "start_time": "2021-10-13T21:51:29.726Z"
    }
   },
   "outputs": [],
   "source": [
    "## Determining the zip codes with the highest ROI those which minimize risk\n",
    "\n",
    "results_roi = best_roi_df.iloc[:5]\n",
    "results_risk = risk_df.iloc[:5]\n",
    "\n",
    "display(results_roi, results_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.284633Z",
     "start_time": "2021-10-13T21:51:29.730Z"
    }
   },
   "outputs": [],
   "source": [
    "## Focusing on the non-shared results\n",
    "display(results_roi.iloc[3:])\n",
    "display(results_risk.iloc[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.286625Z",
     "start_time": "2021-10-13T21:51:29.735Z"
    }
   },
   "outputs": [],
   "source": [
    "## Collecting unique zip codes between roi/risk dataframes\n",
    "zip_set = set()\n",
    "\n",
    "for i in results_roi.index:\n",
    "    zip_set.add(i)\n",
    "    \n",
    "for i in results_risk.index:\n",
    "    zip_set.add(i)\n",
    "    \n",
    "zip_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.287628Z",
     "start_time": "2021-10-13T21:51:29.739Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Concatenating dataframes directly\n",
    "\n",
    "# zip_set = pd.concat([results_roi, results_risk], axis=0).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.288626Z",
     "start_time": "2021-10-13T21:51:29.743Z"
    }
   },
   "outputs": [],
   "source": [
    "## Determining the prices AT THE MIN. FRCST THRSHLD\n",
    "top_results_prices = []\n",
    "\n",
    "for i in zip_set:\n",
    "    top_results_prices.append(price_df.loc[i,:].to_frame().T)\n",
    "    \n",
    "top_prices_df = pd.concat(top_results_prices)\n",
    "\n",
    "top_prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.289639Z",
     "start_time": "2021-10-13T21:51:29.747Z"
    }
   },
   "outputs": [],
   "source": [
    "overall_results[zipcode]['model_visuals']['full']['figure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T21:51:49.291627Z",
     "start_time": "2021-10-13T21:51:29.751Z"
    }
   },
   "outputs": [],
   "source": [
    "## Saving forecast figures in notebook\n",
    "import os\n",
    "fig_folder = \"./img/\"\n",
    "os.makedirs(fig_folder,exist_ok=True)\n",
    "\n",
    "for zipcode in overall_results:\n",
    "    fig = overall_results[zipcode]['model_visuals']['full']['figure']\n",
    "    fig.savefig(f\"{fig_folder}forecast_for_{zipcode}.png\",dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> My forecasts are limited to a 16-month viewpoint (based on the size of the data used for testing purposes).\n",
    "\n",
    "---\n",
    "\n",
    "> I would recommend short-term buyers to **focus on the following areas:**\n",
    ">  * East Liberty **(zip code 15206, ROI: 42.7%)**\n",
    ">  * Lawrenceville **(15201, ROI: 38.8)**\n",
    ">  * North Shore/Brighton Heights **(15212, ROI: 28.9%)**\n",
    "\n",
    "---\n",
    "\n",
    "> I would recommend for short-term buyers to **avoid the following areas due to high risk of losing money:**\n",
    ">  * Shadyside **(15232, 39.3% risk)**\n",
    ">  * Oakland/North Oakland **(15213, 34.2% risk)**\n",
    ">  * Perry South/Northview Heights/Summer Hill **(15214, 27.4%)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> * Comparing forecasts to actualized sales using updated data from Zillow.\n",
    "> * Exploring a larger range of values for the splitting threshold.\n",
    "> * Identifying and adding exogenous data to support forecasts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "980eb18a21052b9c17debf7b5e50bde2e676226d1d1b2f7e2a15b0c620c5c0ea"
  },
  "kernelspec": {
   "display_name": "Python [conda env:learn_env_clone]",
   "language": "python",
   "name": "conda-env-learn_env_clone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "245px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
